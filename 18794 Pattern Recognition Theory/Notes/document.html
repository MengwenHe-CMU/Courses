<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="HMW-Alexander" />
  <title>18794 Pattern Recognition Theory Prof. Marios Savvides</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
  	MathJax.Hub.Config({
  		TeX: {
  			equationNumbers: {autoNumber: "all"}
  		}
  	});
  </script>
</head>
<body>
<div id="header">
<h1 class="title"><strong>18794 Pattern Recognition Theory<br />
Prof. Marios Savvides</strong></h1>
<h2 class="author">HMW-Alexander</h2>
</div>
<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#decision-theory"><span class="toc-section-number">2</span> Decision Theory</a><ul>
<li><a href="#terms"><span class="toc-section-number">2.1</span> Terms</a></li>
<li><a href="#bayes-rule"><span class="toc-section-number">2.2</span> Bayes Rule</a></li>
<li><a href="#minimum-probability-of-error"><span class="toc-section-number">2.3</span> Minimum Probability of Error</a></li>
<li><a href="#likelihood-ratio"><span class="toc-section-number">2.4</span> Likelihood Ratio</a><ul>
<li><a href="#likelihood-as-gaussian-distribution"><span class="toc-section-number">2.4.1</span> Likelihood as Gaussian distribution</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<hr />
<h1 id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<hr />
<h1 id="decision-theory"><span class="header-section-number">2</span> Decision Theory</h1>
<h2 id="terms"><span class="header-section-number">2.1</span> Terms</h2>
<ul>
<li><p>Feature Space:</p>
<ul>
<li><p><strong>Feature</strong>: a distinctive characteristic or quality of the object</p></li>
<li><p><strong>Feature vector</strong>: combine more than one feature as a vector</p></li>
<li><p><strong>Feature space</strong>: The space defined by the feature vectors</p></li>
</ul></li>
<li><p>Classifiers:</p>
<ul>
<li><p><strong>Decision regions</strong>: a classifier partitions the feature space into class-corresponding decision regions.</p></li>
<li><p><strong>Decision boundaries</strong>: the borders between the decision regions.</p></li>
</ul></li>
</ul>
<h2 id="bayes-rule"><span class="header-section-number">2.2</span> Bayes Rule</h2>
<p><span class="math display">\[P(w_i|x)=\frac{P(x,w_i)}{P(x)}=\frac{P(x|w_i)P(w_i)}{\sum_{k=1}^{C}{P(x|w_k)P(w_k)}}\]</span></p>
<ul>
<li><p><strong>Posterior Probability</strong> <span class="math inline">\(P(w_i|x)\)</span>: the conditional probability of correct class being <span class="math inline">\(w_i\)</span> given that feature value <span class="math inline">\(x\)</span> has been observed.</p></li>
<li><p><strong>Evidence</strong> <span class="math inline">\(P(x)\)</span>: the total probability of observing the feature value of <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong>Likelihood</strong> <span class="math inline">\(P(x|w_i)\)</span>: the conditional probability of observing a feature value of <span class="math inline">\(x\)</span> given that the correct class is <span class="math inline">\(w_i\)</span>.</p></li>
<li><p><strong>Prior Probability</strong> <span class="math inline">\(P(w_i)\)</span>: the probability of class <span class="math inline">\(w_i\)</span>, <span class="math inline">\(\sum_{k=1}^{C}{P(w_k)}=1\)</span>.</p></li>
<li><p><strong>Bayes Classifiers</strong> decide on the class that has the <strong>largest posterior probability</strong> (<span class="math inline">\(\max_{w_i}{P(w_i|x)}\)</span>). They are statistically the best classifiers i.e. they are minimum error classifiers (optimal).</p></li>
</ul>
<h2 id="minimum-probability-of-error"><span class="header-section-number">2.3</span> Minimum Probability of Error</h2>
<ul>
<li><p><span class="math inline">\(\epsilon=P(error|class)\)</span>: probability of assigning <span class="math inline">\(x\)</span> to the wrong class <span class="math inline">\(w\)</span>.</p></li>
<li><p><span class="math inline">\(P_e=\sum_{k=1}^{C}{P(w_k)\epsilon_k}\)</span>: total probability of error.</p></li>
</ul>
<p><img src="./img/minimum_probability_of_error.png" alt="image" width="377" /></p>
<p>For the two class case shown above, we want to minimize <span class="math inline">\(P_e\)</span> as below: <span class="math display">\[\begin{array}{rcl}
P_e &amp; = &amp; P(w_1)\epsilon_1 + P(w_2)\epsilon_2 \\
    &amp; = &amp; P(w_1)\int_{R_2}{f(x|w_1)dx} + P(w_2)\int_{R_1}{f(x|w_2)dx} \\
    &amp; = &amp; P(w_1)(1-\int_{R_1}{f(x|w_1)dx}) + P(w_2)\int_{R_1}{f(x|w_2)dx} \\
    &amp; = &amp; P(w_1) + \int_{R_1}{(P(w_2)f(x|w_2) - P(w_1)f(x|w_1))dx} \\ 
\end{array}\]</span> To minimize <span class="math inline">\(P_e\)</span>, we want <span class="math inline">\(P(w_2)f(x|w_2) - P(w_1)f(x|w_1)\)</span> to be always negative<span class="math inline">\((&lt;0)\)</span> in the region <span class="math inline">\(R_1\)</span>: <span class="math display">\[\begin{array}{rcl}
P(w_1)f(x|w_1) - P(w_2)f(x|w_2) &gt;0 &amp; \Rightarrow &amp; w_1 \\
P(w_1)f(x|w_1) - P(w_2)f(x|w_2) &lt;0 &amp; \Rightarrow &amp; w_2 \\
\end{array}\]</span></p>
<h2 id="likelihood-ratio"><span class="header-section-number">2.4</span> Likelihood Ratio</h2>
<ul>
<li><p><strong>Likelihood ratio</strong>: <span class="math inline">\(l(x)=\frac{f(x|w_1)}{f(x|w_2)}\)</span></p></li>
<li><p><strong>Log likelihood ratio</strong>: <span class="math inline">\(ln(l(x))=ln(\frac{f(x|w_1)}{f(x|w_2)})=ln(f(x|w_1))-ln(f(x|w_2))\)</span></p></li>
<li><p><strong>Ratio of a priori probabilities</strong>: <span class="math inline">\(T=\frac{P(w_2)}{P(w_1)}\)</span></p></li>
<li><p><strong>Log ratio of a priori probabilities</strong>: <span class="math inline">\(ln(T)=ln(\frac{P(w_2)}{P(w_1)})=ln(P(w_2))-ln(P(w_1))\)</span></p></li>
</ul>
<p><span class="math display">\[\begin{array}{rcl}
ln(l(x)) = ln(\frac{f(x|w_1)}{f(x|w_2)}) &gt; ln(\frac{P(w_2)}{P(w_1)}) = ln(T) &amp; \Rightarrow &amp; w_1 \\
ln(l(x)) = ln(\frac{f(x|w_1)}{f(x|w_2)}) &lt; ln(\frac{P(w_2)}{P(w_1)}) = ln(T) &amp; \Rightarrow &amp; w_2 \\
\end{array}\]</span></p>
<h3 id="likelihood-as-gaussian-distribution"><span class="header-section-number">2.4.1</span> Likelihood as Gaussian distribution</h3>
<p>Assume likelihood <span class="math inline">\(f(x|w_i)\)</span> are Gaussian distributions with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma_i^2\)</span>. <span class="math display">\[f(x|w_i)=\frac{1}{\sqrt{2\pi\sigma_i^2}}\exp\left(-\frac{(x-\mu_i)^2}{2\sigma_i^2}\right)\]</span></p>
<p><img src="./img/gaussian_distribution_likelihood.png" alt="image" width="226" /></p>
<p>The log likelihood ratio: <span class="math display">\[\begin{array}{rcl}
ln(l(x)) &amp; = &amp; ln\left(\frac{\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x-\mu_1)^2}{2\sigma_1^2}\right)}{\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x-\mu_2)^2}{2\sigma_2^2}\right)}\right) \\
         &amp; = &amp; ln(\frac{\sigma_2}{\sigma_1})+\frac{(x-\mu_2)^2}{2\sigma_2^2}-\frac{(x-\mu_1)^2}{2\sigma_1^2}
\end{array}\]</span></p>
<p>Case: <span class="math inline">\(\sigma_1=\sigma_2=\sigma\)</span> <span class="math display">\[\begin{array}{rcl}
ln(l(x)) &amp; = &amp; \frac{2x(\mu_1-\mu_2)-(\mu_1^2-\mu_2^2)}{2\sigma^2}
\end{array}\]</span></p>
<p><span class="math display">\[\begin{array}{rcl}
x(\mu_1-\mu_2) - \frac{\mu_1^2-\mu_2^2}{2} &gt; \sigma^2 ln(\frac{P(w_2)}{P(w_1)}) &amp; \Rightarrow &amp; w_1 \\
x(\mu_1-\mu_2) - \frac{\mu_1^2-\mu_2^2}{2} &lt; \sigma^2 ln(\frac{P(w_2)}{P(w_1)}) &amp; \Rightarrow &amp; w_2 \\
\end{array}\]</span></p>
<p>If <span class="math inline">\(P(w_1)=P(w_2)\)</span> <span class="math display">\[x = \frac{\mu_1+\mu_2}{2}\]</span></p>
<p><img src="./img/gaussian_linear_classifier.png" alt="image" width="302" /></p>
<p>Case: <span class="math inline">\(\sigma_1\neq\sigma_2\)</span></p>
<p><img src="./img/guassian_quadratic_classifier.png" alt="image" width="302" /></p>
</body>
</html>
